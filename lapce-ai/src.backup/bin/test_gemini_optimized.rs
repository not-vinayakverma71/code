/// TEST OPTIMIZED GEMINI PROVIDER MEMORY USAGE
/// Verifies that optimizations meet < 8MB requirement

use std::sync::{Arc, atomic::{AtomicU64, AtomicBool, Ordering}};
use std::time::{Duration, Instant};
use std::fs;
use anyhow::Result;
use colored::Colorize;
use tokio::task::JoinSet;

use lapce_ai_rust::ai_providers::{
    core_trait::{AiProvider, ChatRequest, ChatMessage},
    gemini_exact::{GeminiProvider, GeminiConfig},
    gemini_optimized::{OptimizedGeminiProvider, OptimizedGeminiConfig},
};

fn get_rss_kb() -> u64 {
    if let Ok(statm) = fs::read_to_string("/proc/self/statm") {
        let parts: Vec<&str> = statm.split_whitespace().collect();
        if parts.len() > 1 {
            let rss_pages = parts[1].parse::<u64>().unwrap_or(0);
            return rss_pages * 4; // 4KB per page
        }
    }
    0
}

async fn measure_provider_memory<P: AiProvider>(
    provider: Arc<P>,
    name: &str,
) -> Result<(u64, u64, u64)> {
    // Baseline before any operations
    let baseline_kb = get_rss_kb();
    tokio::time::sleep(Duration::from_millis(100)).await;
    
    // Make some requests to trigger initialization
    let mut handles = vec![];
    for i in 0..10 {
        let provider = provider.clone();
        let handle = tokio::spawn(async move {
            let request = ChatRequest {
                model: "gemini-2.5-flash".to_string(),
                messages: vec![ChatMessage {
                    role: "user".to_string(),
                    content: Some(format!("Test message {}", i)),
                    name: None,
                    function_call: None,
                    tool_calls: None,
                }],
                temperature: Some(0.0),
                max_tokens: Some(10),
                stream: Some(false),
                top_p: None,
                stop: None,
                presence_penalty: None,
                frequency_penalty: None,
                user: None,
                functions: None,
                function_call: None,
                tools: None,
                tool_choice: None,
                response_format: None,
                seed: None,
                logprobs: None,
                top_logprobs: None,
            };
            
            // Try to make request (may fail without API key)
            let _ = provider.chat(request).await;
        });
        handles.push(handle);
    }
    
    // Wait for all requests
    for handle in handles {
        let _ = handle.await;
    }
    
    // Peak memory after requests
    let peak_kb = get_rss_kb();
    
    // Force some cleanup
    drop(provider);
    tokio::time::sleep(Duration::from_millis(500)).await;
    
    // Final memory
    let final_kb = get_rss_kb();
    
    println!("\n{}", format!("üìä {} MEMORY PROFILE", name).bright_blue().bold());
    println!("{}", "=".repeat(50).bright_blue());
    println!("‚Ä¢ Baseline: {:.2} MB", baseline_kb as f64 / 1024.0);
    println!("‚Ä¢ Peak: {:.2} MB", peak_kb as f64 / 1024.0);
    println!("‚Ä¢ Final: {:.2} MB", final_kb as f64 / 1024.0);
    println!("‚Ä¢ Growth: {:.2} MB", (peak_kb - baseline_kb) as f64 / 1024.0);
    
    Ok((baseline_kb, peak_kb, peak_kb - baseline_kb))
}

#[tokio::main]
async fn main() -> Result<()> {
    dotenv::dotenv().ok();
    
    println!("\n{}", "üî¨ GEMINI PROVIDER MEMORY OPTIMIZATION TEST".bright_blue().bold());
    println!("{}", "=".repeat(60).bright_blue());
    
    let api_key = std::env::var("GEMINI_API_KEY")
        .unwrap_or_else(|_| "test_key".to_string());
    
    // Test 1: Original Gemini Provider
    println!("\n{}", "1Ô∏è‚É£ TESTING ORIGINAL GEMINI PROVIDER".bright_cyan().bold());
    
    let original_config = GeminiConfig {
        api_key: api_key.clone(),
        base_url: Some("https://generativelanguage.googleapis.com".to_string()),
        default_model: Some("gemini-2.5-flash".to_string()),
        api_version: Some("v1beta".to_string()),
        timeout_ms: Some(30000),
        project_id: None,
        location: None,
    };
    
    let original_provider = Arc::new(GeminiProvider::new(original_config).await?);
    let (orig_baseline, orig_peak, orig_growth) = 
        measure_provider_memory(original_provider, "ORIGINAL GEMINI").await?;
    
    // Test 2: Optimized Gemini Provider
    println!("\n{}", "2Ô∏è‚É£ TESTING OPTIMIZED GEMINI PROVIDER".bright_cyan().bold());
    
    let optimized_config = OptimizedGeminiConfig {
        api_key: api_key.clone(),
        base_url: Some("https://generativelanguage.googleapis.com".to_string()),
        default_model: Some("gemini-2.5-flash".to_string()),
        api_version: Some("v1beta".to_string()),
        timeout_ms: Some(30000),
        enable_pooling: true,
        lazy_load: true,
    };
    
    let optimized_provider = Arc::new(OptimizedGeminiProvider::new(optimized_config).await?);
    let (opt_baseline, opt_peak, opt_growth) = 
        measure_provider_memory(optimized_provider, "OPTIMIZED GEMINI").await?;
    
    // Comparison
    println!("\n{}", "üìä OPTIMIZATION RESULTS".bright_green().bold());
    println!("{}", "=".repeat(60).bright_green());
    
    let reduction_mb = (orig_growth as f64 - opt_growth as f64) / 1024.0;
    let reduction_pct = ((orig_growth - opt_growth) as f64 / orig_growth as f64) * 100.0;
    
    println!("‚Ä¢ Original Growth: {:.2} MB", orig_growth as f64 / 1024.0);
    println!("‚Ä¢ Optimized Growth: {:.2} MB", opt_growth as f64 / 1024.0);
    println!("‚Ä¢ Memory Saved: {:.2} MB ({:.1}%)", reduction_mb, reduction_pct);
    
    if opt_growth < 8192 { // 8MB in KB
        println!("\n{}", "‚úÖ OPTIMIZATION SUCCESSFUL!".bright_green().bold());
        println!("{}", "Optimized provider meets < 8MB requirement".bright_green());
    } else {
        println!("\n{}", "‚ö†Ô∏è FURTHER OPTIMIZATION NEEDED".bright_yellow().bold());
        println!("{}", format!("Current: {:.2} MB, Target: < 8 MB", 
            opt_growth as f64 / 1024.0).bright_yellow());
    }
    
    // Test 3: Stress test optimized provider
    println!("\n{}", "3Ô∏è‚É£ STRESS TESTING OPTIMIZED PROVIDER".bright_cyan().bold());
    println!("   Running 100 concurrent requests...");
    
    let optimized_config = OptimizedGeminiConfig {
        api_key,
        base_url: Some("https://generativelanguage.googleapis.com".to_string()),
        default_model: Some("gemini-2.5-flash".to_string()),
        api_version: Some("v1beta".to_string()),
        timeout_ms: Some(30000),
        enable_pooling: true,
        lazy_load: true,
    };
    
    let stress_provider = Arc::new(OptimizedGeminiProvider::new(optimized_config).await?);
    let stress_baseline = get_rss_kb();
    
    let mut tasks = JoinSet::new();
    for i in 0..100 {
        let provider = stress_provider.clone();
        tasks.spawn(async move {
            let request = ChatRequest {
                model: "gemini-2.5-flash".to_string(),
                messages: vec![ChatMessage {
                    role: "user".to_string(),
                    content: Some(format!("Stress test {}", i)),
                    name: None,
                    function_call: None,
                    tool_calls: None,
                }],
                temperature: Some(0.0),
                max_tokens: Some(10),
                stream: Some(false),
                top_p: None,
                stop: None,
                presence_penalty: None,
                frequency_penalty: None,
                user: None,
                functions: None,
                function_call: None,
                tools: None,
                tool_choice: None,
                response_format: None,
                seed: None,
                logprobs: None,
                top_logprobs: None,
            };
            
            let _ = provider.chat(request).await;
        });
    }
    
    while let Some(_) = tasks.join_next().await {}
    
    let stress_peak = get_rss_kb();
    let stress_growth = stress_peak - stress_baseline;
    
    println!("   ‚Ä¢ Baseline: {:.2} MB", stress_baseline as f64 / 1024.0);
    println!("   ‚Ä¢ Peak: {:.2} MB", stress_peak as f64 / 1024.0);
    println!("   ‚Ä¢ Growth: {:.2} MB", stress_growth as f64 / 1024.0);
    
    if stress_growth < 8192 {
        println!("   {} Stress test passed!", "‚úÖ".green());
    } else {
        println!("   {} Stress test shows memory pressure", "‚ö†Ô∏è".yellow());
    }
    
    // Summary
    println!("\n{}", "=".repeat(60).bright_blue());
    println!("{}", "üìä FINAL ASSESSMENT".bright_blue().bold());
    println!("{}", "=".repeat(60).bright_blue());
    
    println!("\n{}", "Optimization Techniques Applied:".bright_cyan());
    println!("‚Ä¢ {} Object pooling for buffers", "‚úÖ".green());
    println!("‚Ä¢ {} Lazy loading of components", "‚úÖ".green());
    println!("‚Ä¢ {} Minimal client initialization", "‚úÖ".green());
    println!("‚Ä¢ {} Reduced model metadata", "‚úÖ".green());
    println!("‚Ä¢ {} Connection pool optimization", "‚úÖ".green());
    
    let final_memory = get_rss_kb();
    println!("\n‚Ä¢ Final Process Memory: {:.2} MB", final_memory as f64 / 1024.0);
    
    if final_memory < 16384 { // 16MB total process
        println!("\n{}", "‚úÖ GEMINI OPTIMIZATION COMPLETE!".bright_green().bold());
        println!("{}", "Provider now meets memory requirements".bright_green());
    } else {
        println!("\n{}", "‚ö†Ô∏è Additional optimization may be needed".bright_yellow());
    }
    
    Ok(())
}
