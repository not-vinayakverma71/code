// PRODUCTION AWS TITAN PERFORMANCE TEST - 100 FILES
use lancedb::search::{SemanticSearchEngine, SearchConfig, HybridSearcher};
use lancedb::search::semantic_search_engine::SearchFilters;
use lancedb::embeddings::service_factory::{IEmbedder, EmbeddingResponse};
use lancedb::database::config_manager::{CodeIndexConfigManager, EmbedderProvider, CodeIndexConfig};
use lancedb::embeddings::service_factory::CodeIndexServiceFactory;
use lancedb::embeddings::aws_titan_production::{AwsTitanProduction, AwsTier, UsageMetrics};
use std::sync::Arc;
use std::time::Instant;
use tempfile::TempDir;
use tokio::fs;
use std::path::PathBuf;
use std::collections::HashMap;

// Test configuration for production embedder
struct ProductionTestConfig {
    embedder: Arc<AwsTitanProduction>,
    name: String,
    expected_dimension: usize,
    files_to_test: usize,
}

#[tokio::test]
async fn test_production_aws_titan_100_files() {
    println!("\nüéØ ===============================================");
    println!("   PRODUCTION AWS TITAN TEST - 100 FILES");
    println!("   ENTERPRISE FEATURES ENABLED");
    println!("   ===============================================\n");

    // Load real credentials from environment
    dotenv::dotenv().ok();

    // Setup directories
    let temp_dir = TempDir::new().unwrap();
    let test_repo = temp_dir.path().join("production_test_repo");
    let db_path = temp_dir.path().join("lancedb_production");

    // Create production-ready AWS Titan embedder
    println!("üè≠ Initializing Production AWS Titan Embedder...");
    let embedder_start = Instant::now();
    let embedder = Arc::new(
        AwsTitanProduction::new("us-east-1", AwsTier::Standard)
            .await
            .expect("Failed to create production AWS Titan embedder")
    );
    println!("   ‚úÖ Production embedder initialized in {:?}", embedder_start.elapsed());

    // Validate embedder
    println!("\nüîê Validating AWS Titan connection...");
    let validation_start = Instant::now();
    let (valid, msg) = embedder.validate_configuration().await
        .unwrap_or((false, Some("Validation failed".to_string())));

    if !valid {
        panic!("AWS Titan validation failed: {}", msg.unwrap_or_default());
    }
    println!("   ‚úÖ Validated in {:?}: {}", validation_start.elapsed(), msg.unwrap_or_default());

    // Create 100 production-grade test files
    println!("\nüìÅ Creating 100 production-grade test files...");
    let file_creation_start = Instant::now();
    create_production_test_repository(&test_repo, 100).await;
    println!("   ‚úÖ Created 100 files in {:?}", file_creation_start.elapsed());

    // Test configuration
    let test_config = ProductionTestConfig {
        embedder: embedder.clone(),
        name: "AWS Titan Production".to_string(),
        expected_dimension: 1536,
        files_to_test: 100,
    };

    // Run comprehensive performance test
    let results = run_production_performance_test(&test_config, &test_repo, &db_path).await;

    // Display final results
    println!("\nüéØ ===============================================");
    println!("   FINAL PRODUCTION PERFORMANCE REPORT");
    println!("   ===============================================");
    println!("   üìä INDEXING RESULTS:");
    println!("      Files processed: {}", results.files_processed);
    println!("      Total chunks: {}", results.total_chunks);
    println!("      API calls made: {}", results.api_calls);
    println!("      Cache hits: {}", results.cache_hits);
    println!("      Index speed: {:.2} chunks/sec", results.chunks_per_second);
    println!("      Avg chunk time: {:.2}ms", results.avg_chunk_time_ms);
    println!("      P95 chunk time: {:.2}ms", results.p95_chunk_time_ms);
    println!("      P99 chunk time: {:.2}ms", results.p99_chunk_time_ms);

    println!("\n   üîç QUERY RESULTS:");
    println!("      Queries executed: {}", results.queries_executed);
    println!("      Avg query time: {:.2}ms", results.avg_query_time_ms);
    println!("      P95 query time: {:.2}ms", results.p95_query_time_ms);
    println!("      P99 query time: {:.2}ms", results.p99_query_time_ms);
    println!("      Queries per second: {:.2}", results.queries_per_second);

    println!("\n   üí∞ COST & EFFICIENCY:");
    println!("      Total tokens: {}", results.total_tokens);
    println!("      Total cost: ${:.4}", results.total_cost_usd);
    println!("      Cost per 1K tokens: ${:.4}", results.cost_per_1k_tokens);
    println!("      Cache hit rate: {:.1}%", results.cache_hit_rate);
    println!("      Retry rate: {:.1}%", results.retry_rate);

    println!("\n   ‚ö° PERFORMANCE RATINGS:");
    println!("      Index throughput: {} chunks/sec", results.index_throughput_rating);
    println!("      Query latency: {} ms", results.query_latency_rating);
    println!("      Cost efficiency: {}", results.cost_efficiency_rating);
    println!("      Overall grade: {}", results.overall_grade);

    // Export detailed metrics
    let metrics_json = embedder.export_metrics().await;
    println!("\nüìà DETAILED METRICS (JSON):");
    println!("{}", metrics_json);

    // Validate performance meets production requirements
    assert!(results.chunks_per_second > 5.0, "Index throughput too slow: {:.2} chunks/sec", results.chunks_per_second);
    assert!(results.avg_query_time_ms < 2000.0, "Query latency too high: {:.2}ms", results.avg_query_time_ms);
    assert!(results.cache_hit_rate > 10.0, "Cache hit rate too low: {:.1}%", results.cache_hit_rate);

    println!("\n‚úÖ PRODUCTION TEST PASSED - AWS Titan is ready for production!");
}

struct TestResults {
    files_processed: usize,
    total_chunks: usize,
    api_calls: usize,
    cache_hits: usize,
    chunks_per_second: f64,
    avg_chunk_time_ms: f64,
    p95_chunk_time_ms: f64,
    p99_chunk_time_ms: f64,
    queries_executed: usize,
    avg_query_time_ms: f64,
    p95_query_time_ms: f64,
    p99_query_time_ms: f64,
    queries_per_second: f64,
    total_tokens: usize,
    total_cost_usd: f64,
    cost_per_1k_tokens: f64,
    cache_hit_rate: f64,
    retry_rate: f64,
    index_throughput_rating: String,
    query_latency_rating: String,
    cost_efficiency_rating: String,
    overall_grade: String,
}

async fn run_production_performance_test(
    config: &ProductionTestConfig,
    test_repo: &PathBuf,
    db_path: &PathBuf,
) -> TestResults {
    println!("\nüöÄ Starting Production Performance Test");
    println!("   ======================================");

    // Create search configuration
    let search_config = SearchConfig {
        db_path: db_path.to_str().unwrap().to_string(),
        cache_size: 1000,
        cache_ttl: 300,
        batch_size: 10,
        max_embedding_dim: Some(config.expected_dimension),
        index_nprobes: Some(10),
        optimal_batch_size: Some(10),
        max_results: 100,
        min_score: 0.5,
    };

    // Initialize semantic search engine
    let engine_start = Instant::now();
    let semantic_engine = Arc::new(
        SemanticSearchEngine::new(search_config.clone(), config.embedder.clone()).await
            .expect("Failed to create semantic engine")
    );
    println!("   ‚úÖ Engine initialized in {:?}", engine_start.elapsed());

    // Performance test: Index files
    println!("\n   üìä INDEXING PERFORMANCE TEST");
    let files_to_index = config.files_to_test;
    let mut index_times = Vec::new();
    let mut total_chunks = 0;
    let mut total_api_calls = 0;
    let mut cache_hits = 0;
    let mut retry_count = 0;

    let index_start = Instant::now();

    for (i, entry) in std::fs::read_dir(test_repo).unwrap().take(files_to_index).enumerate() {
        let entry = entry.unwrap();
        let path = entry.path();

        if path.extension().map(|e| e == "rs").unwrap_or(false) {
            let file_content = std::fs::read_to_string(&path).unwrap();

            // Create realistic code chunks
            let chunks = create_code_chunks(&file_content);
            let chunk_count = chunks.len();

            // Measure embedding time
            let embed_start = Instant::now();
            let embedding_result = config.embedder.create_embeddings(chunks.clone(), None).await;
            let embed_time = embed_start.elapsed();
            index_times.push(embed_time);

            match embedding_result {
                Ok(_) => {
                    total_chunks += chunk_count;
                    // Estimate API calls (with caching, this will be less than chunks)
                    total_api_calls += (chunk_count as f64 * 0.7) as usize; // Assume 70% cache miss rate initially
                }
                Err(e) => {
                    println!("   ‚ö†Ô∏è Embedding error for file {}: {}", i, e);
                    retry_count += 1;
                }
            }

            if (i + 1) % 10 == 0 {
                println!("   üìà Progress: {} files ({} chunks, {} API calls)...",
                    i + 1, total_chunks, total_api_calls);
            }
        }
    }
    let total_index_time = index_start.elapsed();

    // Calculate indexing statistics
    let avg_chunk_time = index_times.iter()
        .map(|d| d.as_secs_f64() * 1000.0)
        .sum::<f64>() / index_times.len() as f64;

    let mut sorted_chunk_times: Vec<f64> = index_times.iter()
        .map(|d| d.as_secs_f64() * 1000.0)
        .collect();
    sorted_chunk_times.sort_by(|a, b| a.partial_cmp(b).unwrap());

    let p95_chunk_time = sorted_chunk_times[(sorted_chunk_times.len() as f64 * 0.95) as usize];
    let p99_chunk_time = sorted_chunk_times[(sorted_chunk_times.len() as f64 * 0.99) as usize];

    let chunks_per_second = total_chunks as f64 / total_index_time.as_secs_f64();

    // Performance test: Queries
    println!("\n   üîç QUERY PERFORMANCE TEST");
    let test_queries = vec![
        "async function implementation",
        "error handling patterns",
        "trait implementation",
        "struct with generics",
        "match statement",
        "vector operations",
        "hashmap usage",
        "tokio runtime",
        "serde serialization",
        "Result type handling",
        "database connection",
        "API client implementation",
        "middleware pattern",
        "authentication system",
        "caching strategy",
        "configuration management",
        "logging implementation",
        "testing framework",
        "performance optimization",
        "memory management",
    ];

    let mut query_times = Vec::new();
    for (i, query) in test_queries.iter().enumerate() {
        let query_start = Instant::now();

        // Create embedding for query
        let query_embedding_result = config.embedder.create_embeddings(
            vec![query.to_string()],
            None
        ).await;

        let query_time = query_start.elapsed();
        query_times.push(query_time);

        match query_embedding_result {
            Ok(_) => {
                if (i + 1) % 5 == 0 {
                    println!("      Query {}: {:?}", i + 1, query_time);
                }
            }
            Err(e) => {
                println!("      Query '{}' failed: {}", query, e);
            }
        }
    }

    // Calculate query statistics
    let avg_query_time = query_times.iter()
        .map(|d| d.as_secs_f64() * 1000.0)
        .sum::<f64>() / query_times.len() as f64;

    let mut sorted_query_times: Vec<f64> = query_times.iter()
        .map(|d| d.as_secs_f64() * 1000.0)
        .collect();
    sorted_query_times.sort_by(|a, b| a.partial_cmp(b).unwrap());

    let p95_query_time = sorted_query_times[(sorted_query_times.len() as f64 * 0.95) as usize];
    let p99_query_time = sorted_query_times[(sorted_query_times.len() as f64 * 0.99) as usize];

    let queries_per_second = 1000.0 / avg_query_time;

    // Get final metrics
    let metrics = config.embedder.get_metrics().await;

    // Calculate derived metrics
    let total_tokens = metrics.total_tokens;
    let total_cost_usd = metrics.total_cost_usd;
    let cost_per_1k_tokens = if total_tokens > 0 { (total_cost_usd / total_tokens as f64) * 1000.0 } else { 0.0 };
    let cache_hit_rate = if metrics.total_requests > 0 { (metrics.cached_requests as f64 / metrics.total_requests as f64) * 100.0 } else { 0.0 };
    let retry_rate = if metrics.successful_requests > 0 { (metrics.retry_count as f64 / metrics.successful_requests as f64) * 100.0 } else { 0.0 };

    // Performance ratings
    let index_throughput_rating = if chunks_per_second > 20.0 { "EXCELLENT" }
        else if chunks_per_second > 10.0 { "GOOD" }
        else if chunks_per_second > 5.0 { "ACCEPTABLE" }
        else { "NEEDS IMPROVEMENT" };

    let query_latency_rating = format!("{:.0}ms", avg_query_time);

    let cost_efficiency_rating = if cost_per_1k_tokens < 0.01 { "EXCELLENT" }
        else if cost_per_1k_tokens < 0.02 { "GOOD" }
        else if cost_per_1k_tokens < 0.05 { "ACCEPTABLE" }
        else { "EXPENSIVE" };

    let overall_grade = if chunks_per_second > 10.0 && avg_query_time < 1000.0 && cache_hit_rate > 20.0 {
        "A+ (Production Ready)"
    } else if chunks_per_second > 5.0 && avg_query_time < 2000.0 && cache_hit_rate > 10.0 {
        "B (Good for Production)"
    } else if chunks_per_second > 2.0 && avg_query_time < 5000.0 {
        "C (Needs Optimization)"
    } else {
        "D (Not Production Ready)"
    };

    TestResults {
        files_processed: files_to_index,
        total_chunks,
        api_calls: metrics.successful_requests,
        cache_hits: metrics.cached_requests,
        chunks_per_second,
        avg_chunk_time_ms: avg_chunk_time,
        p95_chunk_time_ms: p95_chunk_time,
        p99_chunk_time_ms: p99_chunk_time,
        queries_executed: test_queries.len(),
        avg_query_time_ms: avg_query_time,
        p95_query_time_ms: p95_query_time,
        p99_query_time_ms: p99_query_time,
        queries_per_second,
        total_tokens,
        total_cost_usd,
        cost_per_1k_tokens,
        cache_hit_rate,
        retry_rate,
        index_throughput_rating: index_throughput_rating.to_string(),
        query_latency_rating,
        cost_efficiency_rating: cost_efficiency_rating.to_string(),
        overall_grade: overall_grade.to_string(),
    }
}

fn create_code_chunks(content: &str) -> Vec<String> {
    // Create realistic code chunks (functions, structs, impls)
    let lines: Vec<&str> = content.lines().collect();
    let mut chunks = Vec::new();
    let chunk_size = 25; // Lines per chunk

    for chunk_start in (0..lines.len()).step_by(chunk_size) {
        let chunk_end = (chunk_start + chunk_size).min(lines.len());
        let chunk = lines[chunk_start..chunk_end].join("\n");
        if !chunk.trim().is_empty() {
            chunks.push(chunk);
        }
    }

    // Ensure we have at least one chunk
    if chunks.is_empty() && !content.trim().is_empty() {
        chunks.push(content.to_string());
    }

    chunks
}

async fn create_production_test_repository(path: &PathBuf, file_count: usize) {
    fs::create_dir_all(path).await.unwrap();

    for i in 0..file_count {
        let content = generate_production_rust_code(i);
        let file_path = path.join(format!("service_{:03}.rs", i));
        fs::write(file_path, content).await.unwrap();
    }

    println!("   ‚úÖ Created {} production-grade Rust service files", file_count);
}

fn generate_production_rust_code(index: usize) -> String {
    format!(r#"
// Service Module {} - Production Quality Code
use std::collections::{{HashMap, BTreeMap, HashSet, VecDeque}};
use std::sync::{{Arc, RwLock, Mutex, atomic::{{AtomicBool, Ordering}}}};
use tokio::sync::{{Semaphore, broadcast, mpsc}};
use serde::{{Serialize, Deserialize}};
use anyhow::{{Result, Context, bail}};
use tracing::{{info, warn, error, debug}};
use uuid::Uuid;
use chrono::{{DateTime, Utc}};

/// Production service configuration
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ServiceConfig {{
    pub service_id: String,
    pub max_connections: usize,
    pub timeout_ms: u64,
    pub retry_attempts: u32,
    pub base_url: String,
    pub cache_ttl: Option<u64>,
    pub enable_compression: bool,
    pub rate_limit_per_minute: u32,
    pub circuit_breaker_threshold: u32,
    pub health_check_interval: u64,
}}

/// Service implementation with enterprise features
pub struct ProductionService {{
    id: Uuid,
    config: Arc<RwLock<ServiceConfig>>,
    cache: Arc<RwLock<HashMap<String, CacheEntry>>>,
    connections: Arc<Mutex<BTreeMap<u64, Connection>>>,
    semaphore: Arc<Semaphore>,
    shutdown: Arc<AtomicBool>,
    metrics: Arc<MetricsCollector>,
    circuit_breaker: Arc<CircuitBreaker>,
}}

#[derive(Debug, Clone)]
struct CacheEntry {{
    data: Vec<u8>,
    expires_at: DateTime<Utc>,
    access_count: u64,
    last_accessed: DateTime<Utc>,
}}

#[derive(Debug, Clone)]
struct Connection {{
    id: u64,
    created_at: DateTime<Utc>,
    last_used: DateTime<Utc>,
    is_healthy: bool,
}}

#[derive(Debug, Clone)]
struct MetricsCollector {{
    total_requests: Arc<AtomicU64>,
    successful_requests: Arc<AtomicU64>,
    failed_requests: Arc<AtomicU64>,
    cache_hits: Arc<AtomicU64>,
    cache_misses: Arc<AtomicU64>,
    avg_response_time: Arc<RwLock<f64>>,
    request_rate: Arc<RwLock<Vec<u64>>>,
}}

impl Default for MetricsCollector {{
    fn default() -> Self {{
        Self {{
            total_requests: Arc::new(AtomicU64::new(0)),
            successful_requests: Arc::new(AtomicU64::new(0)),
            failed_requests: Arc::new(AtomicU64::new(0)),
            cache_hits: Arc::new(AtomicU64::new(0)),
            cache_misses: Arc::new(AtomicU64::new(0)),
            avg_response_time: Arc::new(RwLock::new(0.0)),
            request_rate: Arc::new(RwLock::new(Vec::new())),
        }}
    }}
}}

#[derive(Debug, Clone)]
struct CircuitBreaker {{
    failure_count: Arc<AtomicU32>,
    last_failure_time: Arc<RwLock<Option<DateTime<Utc>>>>,
    state: Arc<AtomicBool>, // false = closed, true = open
}}

impl ProductionService {{
    pub async fn new(config: ServiceConfig) -> Result<Self> {{
        let semaphore = Arc::new(Semaphore::new(config.max_connections));
        let circuit_breaker = Arc::new(CircuitBreaker {{
            failure_count: Arc::new(AtomicU32::new(0)),
            last_failure_time: Arc::new(RwLock::new(None)),
            state: Arc::new(AtomicBool::new(false)),
        }});

        Ok(Self {{
            id: Uuid::new_v4(),
            config: Arc::new(RwLock::new(config)),
            cache: Arc::new(RwLock::new(HashMap::new())),
            connections: Arc::new(Mutex::new(BTreeMap::new())),
            semaphore,
            shutdown: Arc::new(AtomicBool::new(false)),
            metrics: Arc::new(MetricsCollector::default()),
            circuit_breaker,
        }})
    }}

    pub async fn process_request(&self, request: ServiceRequest) -> Result<ServiceResponse> {{
        let start_time = Instant::now();

        // Update metrics
        self.metrics.total_requests.fetch_add(1, Ordering::Relaxed);

        // Check circuit breaker
        if self.circuit_breaker.state.load(Ordering::Relaxed) {{
            return Err(anyhow::anyhow!("Circuit breaker is open"));
        }}

        // Acquire semaphore permit
        let _permit = self.semaphore.acquire().await
            .context("Failed to acquire semaphore")?;

        // Check shutdown signal
        if self.shutdown.load(Ordering::Relaxed) {{
            bail!("Service is shutting down");
        }}

        // Check cache first
        if let Some(cached) = self.check_cache(&request.key).await {{
            self.metrics.cache_hits.fetch_add(1, Ordering::Relaxed);
            let response_time = start_time.elapsed().as_millis() as f64;
            self.update_avg_response_time(response_time).await;
            return Ok(cached);
        }}
        self.metrics.cache_misses.fetch_add(1, Ordering::Relaxed);

        // Process request with retries and circuit breaker
        let result = self.execute_with_retries(&request).await;

        let response_time = start_time.elapsed().as_millis() as f64;
        self.update_avg_response_time(response_time).await;

        match result {{
            Ok(response) => {{
                self.metrics.successful_requests.fetch_add(1, Ordering::Relaxed);
                self.update_cache(&request.key, &response).await;
                Ok(response)
            }}
            Err(e) => {{
                self.metrics.failed_requests.fetch_add(1, Ordering::Relaxed);
                self.handle_failure().await;
                Err(e)
            }}
        }}
    }}

    async fn check_cache(&self, key: &str) -> Option<ServiceResponse> {{
        let cache = self.cache.read().await;

        if let Some(entry) = cache.get(key) {{
            if entry.expires_at > Utc::now() {{
                return Some(ServiceResponse::from_cache(entry.data.clone()));
            }}
        }}

        None
    }}

    async fn update_cache(&self, key: &str, response: &ServiceResponse) {{
        let mut cache = self.cache.write().await;
        let config = self.config.read().await;

        if let Some(ttl) = config.cache_ttl {{
            let expires_at = Utc::now() + chrono::Duration::seconds(ttl as i64);
            cache.insert(key.to_string(), CacheEntry {{
                data: response.to_bytes(),
                expires_at,
                access_count: 1,
                last_accessed: Utc::now(),
            }});
        }}
    }}

    async fn execute_with_retries(&self, request: &ServiceRequest) -> Result<ServiceResponse> {{
        let config = self.config.read().await;
        let mut attempts = 0;

        loop {{
            match self.execute_request_internal(request).await {{
                Ok(response) => return Ok(response),
                Err(e) if attempts < config.retry_attempts => {{
                    attempts += 1;
                    warn!("Request failed, attempt {}/{}: {{:?}}", attempts, config.retry_attempts, e);
                    tokio::time::sleep(std::time::Duration::from_millis(100 * attempts as u64)).await;
                }}
                Err(e) => return Err(e),
            }}
        }}
    }}

    async fn execute_request_internal(&self, request: &ServiceRequest) -> Result<ServiceResponse> {{
        // Simulate complex async processing with monitoring
        tokio::time::sleep(std::time::Duration::from_millis(50)).await;

        // Pattern matching for different request types
        match request.request_type {{
            RequestType::Get => self.handle_get(request).await,
            RequestType::Post => self.handle_post(request).await,
            RequestType::Delete => self.handle_delete(request).await,
            RequestType::Put => self.handle_put(request).await,
            _ => bail!("Unsupported request type"),
        }}
    }}

    async fn handle_get(&self, request: &ServiceRequest) -> Result<ServiceResponse> {{
        Ok(ServiceResponse {{
            id: Uuid::new_v4(),
            data: format!("GET response for {{}}", request.key),
            timestamp: Utc::now(),
            metadata: Default::default(),
        }})
    }}

    async fn handle_post(&self, request: &ServiceRequest) -> Result<ServiceResponse> {{
        Ok(ServiceResponse {{
            id: Uuid::new_v4(),
            data: format!("POST response for {{}}", request.key),
            timestamp: Utc::now(),
            metadata: Default::default(),
        }})
    }}

    async fn handle_delete(&self, request: &ServiceRequest) -> Result<ServiceResponse> {{
        Ok(ServiceResponse {{
            id: Uuid::new_v4(),
            data: format!("DELETE response for {{}}", request.key),
            timestamp: Utc::now(),
            metadata: Default::default(),
        }})
    }}

    async fn handle_put(&self, request: &ServiceRequest) -> Result<ServiceResponse> {{
        Ok(ServiceResponse {{
            id: Uuid::new_v4(),
            data: format!("PUT response for {{}}", request.key),
            timestamp: Utc::now(),
            metadata: Default::default(),
        }})
    }}

    async fn handle_failure(&self) {{
        let failure_count = self.circuit_breaker.failure_count.fetch_add(1, Ordering::Relaxed) + 1;
        *self.circuit_breaker.last_failure_time.write().await = Some(Utc::now());

        let config = self.config.read().await;
        if failure_count >= config.circuit_breaker_threshold {{
            self.circuit_breaker.state.store(true, Ordering::Relaxed);
            warn!("Circuit breaker opened after {{}} failures", failure_count);
        }}
    }}

    async fn update_avg_response_time(&self, response_time: f64) {{
        let mut avg = self.metrics.avg_response_time.write().await;
        let current_avg = *avg;
        let total_requests = self.metrics.total_requests.load(Ordering::Relaxed);

        if total_requests == 1 {{
            *avg = response_time;
        }} else {{
            *avg = (current_avg * (total_requests - 1) as f64 + response_time) / total_requests as f64;
        }}
    }}

    pub async fn get_metrics(&self) -> ServiceMetrics {{
        ServiceMetrics {{
            total_requests: self.metrics.total_requests.load(Ordering::Relaxed),
            successful_requests: self.metrics.successful_requests.load(Ordering::Relaxed),
            failed_requests: self.metrics.failed_requests.load(Ordering::Relaxed),
            cache_hits: self.metrics.cache_hits.load(Ordering::Relaxed),
            cache_misses: self.metrics.cache_misses.load(Ordering::Relaxed),
            avg_response_time: *self.metrics.avg_response_time.read().await,
        }}
    }}

    pub async fn shutdown(&self) {{
        info!("Initiating graceful shutdown for service {{}}", self.id);
        self.shutdown.store(true, Ordering::Relaxed);

        // Wait for ongoing requests
        let config = self.config.read().await;
        let _ = self.semaphore.acquire_many(config.max_connections as u32).await;

        // Clear cache
        self.cache.write().await.clear();

        // Close connections
        self.connections.lock().await.clear();

        info!("Shutdown complete for service {{}}", self.id);
    }}
}}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ServiceRequest {{
    pub key: String,
    pub request_type: RequestType,
    pub payload: Vec<u8>,
    pub metadata: HashMap<String, String>,
}}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub enum RequestType {{
    Get,
    Post,
    Put,
    Delete,
}}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ServiceResponse {{
    pub id: Uuid,
    pub data: String,
    pub timestamp: DateTime<Utc>,
    pub metadata: HashMap<String, String>,
}}

impl ServiceResponse {{
    pub fn from_cache(data: Vec<u8>) -> Self {{
        Self {{
            id: Uuid::new_v4(),
            data: String::from_utf8(data).unwrap_or_default(),
            timestamp: Utc::now(),
            metadata: HashMap::new(),
        }}
    }}

    pub fn to_bytes(&self) -> Vec<u8> {{
        serde_json::to_vec(self).unwrap_or_default()
    }}
}}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ServiceMetrics {{
    pub total_requests: u64,
    pub successful_requests: u64,
    pub failed_requests: u64,
    pub cache_hits: u64,
    pub cache_misses: u64,
    pub avg_response_time: f64,
}}

#[cfg(test)]
mod tests {{
    use super::*;

    #[tokio::test]
    async fn test_service_creation() {{
        let config = ServiceConfig {{
            service_id: "test-service".to_string(),
            max_connections: 10,
            timeout_ms: 5000,
            retry_attempts: 3,
            base_url: "http://localhost".to_string(),
            cache_ttl: Some(60),
            enable_compression: true,
            rate_limit_per_minute: 100,
            circuit_breaker_threshold: 5,
            health_check_interval: 30,
        }};

        let service = ProductionService::new(config).await.unwrap();
        assert_ne!(service.id, Uuid::nil());
    }}

    #[tokio::test]
    async fn test_request_processing() {{
        let config = ServiceConfig {{
            service_id: "test-service".to_string(),
            max_connections: 10,
            timeout_ms: 5000,
            retry_attempts: 3,
            base_url: "http://localhost".to_string(),
            cache_ttl: Some(60),
            enable_compression: true,
            rate_limit_per_minute: 100,
            circuit_breaker_threshold: 5,
            health_check_interval: 30,
        }};

        let service = ProductionService::new(config).await.unwrap();
        let request = ServiceRequest {{
            key: "test_key".to_string(),
            request_type: RequestType::Get,
            payload: vec![],
            metadata: HashMap::new(),
        }};

        let response = service.process_request(request).await.unwrap();
        assert!(!response.data.is_empty());
        assert!(response.data.contains("GET response"));
    }}
}}
"#, index)
}
